# 分类头结构

<cite>
**本文档中引用的文件**  
- [biencoder_embedding_classification_concanated_together.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together.py)
- [biencoder_embedding_classification_with_layernorm_relu.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_with_layernorm_relu.py)
- [biencoder_embedding_classification_without_layernorm.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_without_layernorm.py)
- [biencoder_embedding_classification_with_relu.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_with_relu.py)
- [biencoder_embedding_classification_with_layernorm.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_with_layernorm.py)
- [biencoder_embedding_classification_withlayernorm.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_withlayernorm.py)
- [biencoder_embedding_classification_without_relu.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_without_relu.py)
</cite>

## 目录
1. [分类头结构设计](#分类头结构设计)
2. [特征拼接与输入构建](#特征拼接与输入构建)
3. [可配置变体分析](#可配置变体分析)
4. [分类头与相似度头的协同机制](#分类头与相似度头的协同机制)
5. [扩展方法与调优建议](#扩展方法与调优建议)

## 分类头结构设计

在 `BiModel` 类中，分类头的核心组件是 `linear2` 线性层，定义为 `nn.Linear(2688, 2)`。该层作为二分类器，用于判断论点-反论点对是否匹配。其输入维度为 2688，输出维度为 2，分别对应“匹配”和“不匹配”两个类别。

`linear2` 层位于模型的分类分支，与用于相似度计算的 `linear1` 层（`nn.Linear(768, 128)`）并行存在。`linear1` 负责生成用于检索的低维相似度向量，而 `linear2` 则专注于对论点对进行精确的二元分类决策。

**Section sources**
- [biencoder_embedding_classification_concanated_together.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together.py#L55)
- [biencoder_embedding_classification_with_layernorm_relu.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_with_layernorm_relu.py#L55)

## 特征拼接与输入构建

`classify_pair()` 方法是分类头功能实现的关键。该方法接收两组嵌入向量（`emb1_1`, `emb1_2` 和 `emb2_1`, `emb2_2`），并执行以下步骤构建高维特征输入：

1.  **计算差值**：分别计算两组嵌入向量之间的差值（`x1_diff = x1_1 - x1_2` 和 `x2_diff = x2_1 - x2_2`）。
2.  **取绝对值**：对差值向量取绝对值（`torch.abs(x1_diff)`, `torch.abs(x2_diff)`），以保留差异的幅度信息，消除方向性影响。
3.  **特征拼接**：将原始嵌入向量、差值向量及其绝对值进行拼接。具体拼接顺序为：`[x1_1, x1_2, |x1_diff|, x2_1, x2_2, |x2_diff|]`。
4.  **维度分析**：假设每个 BERT 嵌入向量的维度为 768，则拼接后的总维度为 `768 * 6 = 4608`。然而，代码中 `linear2` 的输入维度为 2688，这表明实际使用的嵌入向量可能来自 BERT 的特定层或经过了降维处理（例如，`linear1` 的输出为 128 维，但 `classify_pair` 的输入是原始 BERT 输出 `x[1]`，即 [CLS] 标记的 768 维向量）。此处的 2688 维度可能是一个特定配置或计算结果，需要结合具体上下文确认。

最终，这个高维特征向量被送入 `linear2` 层进行分类。

**Section sources**
- [biencoder_embedding_classification_concanated_together.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together.py#L64-L73)
- [biencoder_embedding_classification_with_layernorm_relu.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_with_layernorm_relu.py#L64-L73)

## 可配置变体分析

在 `biencoder_second_stage_experiment` 目录下，存在多个变体文件，用于研究不同配置对模型性能的影响。这些变体主要围绕 `LayerNorm` 和 `ReLU` 激活函数的引入：

- **`with_layernorm_relu` 配置**：该配置在 `linear2` 层之后引入了 LayerNorm 和 ReLU 激活函数。LayerNorm 有助于稳定训练过程，通过归一化每一层的输入来减少内部协变量偏移。ReLU 激活函数引入非线性，使模型能够学习更复杂的决策边界。实验表明，这种配置通常能提升模型的泛化能力和最终性能。
- **`without_layernorm` 和 `without_relu` 配置**：这些配置分别移除了 LayerNorm 或 ReLU，用于进行消融研究。通过对比这些变体的性能（如日志文件 `with_layernorm_relu_hard_negatives_0.01_logs.txt` 中的准确率），可以量化 LayerNorm 和 ReLU 对模型效果的贡献。例如，移除 ReLU 可能导致模型表达能力下降，而移除 LayerNorm 可能使训练过程更不稳定。

这些变体文件的代码结构与基础版本高度一致，主要区别在于 `classify_pair` 方法中是否在 `linear2` 后添加了 `nn.LayerNorm` 和 `nn.ReLU` 层。

**Section sources**
- [biencoder_embedding_classification_with_layernorm_relu.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_with_layernorm_relu.py)
- [biencoder_embedding_classification_without_layernorm.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_without_layernorm.py)
- [biencoder_embedding_classification_without_relu.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_without_relu.py)
- [biencoder_embedding_classification_with_layernorm.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_with_layernorm.py)
- [biencoder_embedding_classification_with_relu.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_with_relu.py)

## 分类头与相似度头的协同机制

分类头（`linear2`）与相似度计算头（`linear1`）在 `BiModel` 中协同工作，形成一个双分支架构：

1.  **共享编码器**：`model1`（BERT 模型）作为共享的编码器，为输入文本生成高质量的上下文嵌入。
2.  **并行处理**：
    *   **相似度头**：`linear1` 将 BERT 输出的 [CLS] 向量（768 维）映射到一个低维空间（128 维），生成用于快速检索和粗粒度相似度计算的向量。
    *   **分类头**：`classify_pair` 方法利用原始的 BERT 嵌入向量，通过复杂的特征工程（拼接、差值、绝对值）构建一个高维、信息丰富的特征向量，然后由 `linear2` 进行精细的二元分类。
3.  **功能互补**：这种设计实现了功能上的互补。相似度头适用于大规模检索场景，效率高；而分类头则用于对检索出的候选对进行精确的匹配判断，准确率高。两者结合，可以在保证效率的同时提升最终的匹配精度。

**Section sources**
- [biencoder_embedding_classification_concanated_together.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together.py#L54-L55)
- [biencoder_embedding_classification_with_layernorm_relu.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_with_layernorm_relu.py#L54-L55)

## 扩展方法与调优建议

为了调整分类头的复杂度和性能，可以采用以下扩展方法：

1.  **增加层数**：将单层的 `linear2` 扩展为一个多层感知机（MLP）。例如，可以设计为 `nn.Sequential(nn.Linear(2688, 512), nn.ReLU(), nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, 2))`。增加层数可以提升模型的表达能力，但同时也增加了过拟合的风险和计算成本。
2.  **调整维度**：可以调整 `linear2` 的输入和输出维度。例如，改变特征拼接的方式或在 `linear2` 前加入一个降维层。调整维度需要在模型容量和计算效率之间进行权衡。
3.  **引入更多非线性**：除了 ReLU，还可以尝试其他激活函数，如 GELU 或 Swish，以探索不同的非线性变换效果。
4.  **正则化**：在多层结构中，可以加入 Dropout 层来防止过拟合。
5.  **调优建议**：建议通过消融实验系统地评估不同配置（层数、维度、激活函数、归一化）的影响。使用 `biencoder_second_stage_experiment` 目录下的变体文件作为模板，可以方便地进行此类实验，并通过日志文件分析性能变化。

**Section sources**
- [biencoder_embedding_classification_with_layernorm_relu.py](file://bert/biencoder_second_stage_experiment/biencoder_embedding_classification_with_layernorm_relu.py)
- [biencoder_embedding_classification_concanated_together.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together.py)