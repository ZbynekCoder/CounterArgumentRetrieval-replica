# 随机采样

<cite>
**本文档中引用的文件**  
- [negative_embedding_sampler.py](file://bert/negative_embedding_sampler.py)
- [biencoder_embedding_classification_concanated_together_random.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together_random.py)
- [biencoder_embedding_classification_concanated_together_without_random.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together_without_random.py)
- [biencoder_embedding_classification_decreased_0.01.py](file://bert/model_structure_with_different_decreased_random_rate/biencoder_embedding_classification_decreased_0.01.py)
- [biencoder_embedding_classification_concanated_together.py](file://bert/model_structure_with_simply_random_negative/biencoder_embedding_classification_concanated_together.py)
</cite>

## 目录
1. [引言](#引言)
2. [随机采样策略实现机制](#随机采样策略实现机制)
3. [BallTreeSearcher.search方法中的random_rate控制逻辑](#balltreesearchersearch方法中的random_rate控制逻辑)
4. [纯随机采样触发机制](#纯随机采样触发机制)
5. [防止模型过早收敛于简单负例](#防止模型过早收敛于简单负例)
6. [与其他策略的融合使用模式](#与其他策略的融合使用模式)
7. [计算开销与噪声权衡](#计算开销与噪声权衡)
8. [结论](#结论)

## 引言
在对抗性论点检索任务中，负样本的选择对模型训练效果至关重要。传统的基于距离的负采样方法容易导致模型过早收敛于简单的负例，从而限制了模型的泛化能力。为此，本项目引入了一种随机采样策略，通过在BallTree搜索的基础上引入随机性，增强模型对困难负例的学习能力。该策略在多个biencoder训练脚本中得到了应用，并通过不同的random_rate参数进行调节。

## 随机采样策略实现机制

随机采样策略的核心实现在`negative_embedding_sampler.py`文件中的`BallTreeSearcher`类。该类利用BallTree数据结构对点（point）和反论点（counter）的嵌入向量进行索引，支持高效的最近邻搜索。在每次训练迭代中，系统会根据当前模型生成的嵌入向量重新构建BallTree，并结合随机采样策略选择负样本。

该策略的关键在于`search`方法中`random_rate`参数的使用，它控制了纯随机采样的概率。当随机数小于`random_rate`时，系统将放弃基于距离的搜索结果，转而采用完全随机的方式选择负样本。这种混合策略既保留了基于语义相似度的负采样优势，又通过随机性引入了多样性，防止模型陷入局部最优。

**Section sources**
- [negative_embedding_sampler.py](file://bert/negative_embedding_sampler.py#L13-L46)

## BallTreeSearcher.search方法中的random_rate控制逻辑

`BallTreeSearcher.search`方法接受一个`random_rate`参数（默认值为0.8），用于控制随机采样的比例。该参数在条件判断`if(random.random()>random_rate)`中起作用。值得注意的是，这里的逻辑是“大于”而非“小于”，这意味着当`random_rate`设置为0.8时，有80%的概率触发随机采样。

具体而言，`random.random()`生成一个[0,1)区间内的均匀分布随机数。如果该随机数大于`random_rate`，则执行基于BallTree的最近邻搜索；否则执行纯随机采样。这一设计使得`random_rate`实际上代表了“非随机采样”的概率阈值，而1-`random_rate`才是真正的随机采样概率。

在训练过程中，`random_rate`通常会随着训练轮次逐渐减小，如在`biencoder_embedding_classification_concanated_together_random.py`中所示：`0.8-i*0.02 if 0.8-i*0.02>0 else 0`。这种退火策略使得模型在训练初期更多地依赖随机采样以探索解空间，在后期则更多地依赖语义相似度进行精细优化。

**Section sources**
- [negative_embedding_sampler.py](file://bert/negative_embedding_sampler.py#L20-L46)
- [biencoder_embedding_classification_concanated_together_random.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together_random.py#L199-L202)

## 纯随机采样触发机制

当`random.random() <= random_rate`时，系统将触发纯随机采样机制。此时，负样本的选择完全脱离语义相似度的约束，通过以下方式生成：

```python
negative_index.append((random.randint(0,1), random.randint(0,len(point_index)-1)))
```

该机制首先随机选择样本类别（0或1），其中0代表point类样本，1代表counter类样本。然后在对应类别中随机选择一个样本索引。这种双重随机性确保了负样本的多样性和不可预测性。

值得注意的是，这里的随机采样并非完全无约束。样本索引的范围受限于当前批次中可用的样本数量，且类别选择仅限于两种预定义类型。这种设计在保证随机性的同时，也维持了基本的数据结构一致性。

**Section sources**
- [negative_embedding_sampler.py](file://bert/negative_embedding_sampler.py#L45-L46)

## 防止模型过早收敛于简单负例

随机采样策略通过引入不可预测的负样本，有效防止了模型过早收敛于简单的负例。在传统的基于距离的负采样中，模型往往会快速学会区分明显不同的样本，从而导致训练过程停滞在局部最优解。而随机采样强制模型面对各种可能的负例组合，包括那些语义上不相关甚至无关的样本。

这种机制迫使模型学习更加鲁棒的特征表示，而不是依赖于表面的词汇重叠或简单的语义匹配。通过在训练初期保持较高的随机率，模型能够探索更广阔的解空间，建立更加全面的判别边界。随着训练的进行，逐渐降低随机率使得模型能够在保持泛化能力的同时，对困难负例进行精细化学习。

实验结果表明，采用随机采样策略的模型在验证集和测试集上的准确率均优于仅使用基于距离采样的模型，证明了该策略在提升模型泛化能力方面的有效性。

**Section sources**
- [biencoder_embedding_classification_concanated_together_random.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together_random.py#L207-L210)
- [biencoder_embedding_classification_concanated_together_without_random.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together_without_random.py#L207-L210)

## 与其他策略的融合使用模式

随机采样策略并非孤立存在，而是与其他训练策略紧密结合，形成了一套完整的训练框架。在biencoder主训练脚本中，该策略与以下机制协同工作：

1. **周期性负样本更新**：在每个训练轮次结束后，系统会重新计算所有样本的嵌入向量，并基于最新的模型状态更新负样本索引。这确保了负样本的选择始终反映当前模型的认知水平。

2. **多阶段训练**：通过在不同训练阶段调整`random_rate`，实现了从探索到利用的平滑过渡。初期高随机率促进探索，后期低随机率支持精细优化。

3. **混合损失函数**：随机采样与三元组损失和交叉熵损失相结合，共同指导模型学习。三元组损失强化了正负样本间的距离分离，而交叉熵损失则直接优化分类性能。

4. **模型评估集成**：在每个epoch结束后，使用`BallTreeEvaluater`对模型性能进行评估，为训练过程提供反馈。这种闭环设计使得训练策略能够根据模型表现动态调整。

这种融合使用模式体现了系统化的设计思想，将随机采样作为整体训练策略的一个有机组成部分，而非简单的附加功能。

**Section sources**
- [biencoder_embedding_classification_concanated_together_random.py](file://bert/biencoder/biencoder_embedding_classification_concanated_together_random.py#L175-L206)
- [negative_embedding_sampler.py](file://bert/negative_embedding_sampler.py#L48-L78)

## 计算开销与噪声权衡

随机采样策略具有显著的计算效率优势。相比于复杂的负样本挖掘算法，纯随机采样的时间复杂度为O(1)，几乎不增加额外的计算负担。BallTree的构建和查询虽然有一定开销，但由于其仅在每个epoch开始时执行一次，总体计算成本仍然可控。

然而，这种高效性也带来了潜在的风险——噪声引入。完全随机选择的负样本可能包含大量与正样本毫无关联的"无关噪声"，这可能导致模型学习到错误的关联模式。特别是在训练初期，当模型尚未建立基本的语义理解能力时，过多的噪声可能干扰学习过程。

为平衡这一权衡，项目采用了动态调整`random_rate`的策略。通过在训练初期设置较高的随机率（如0.8），鼓励探索；随着训练进行逐步降低随机率，减少噪声影响。此外，实验对比了不同固定随机率（如0.01）和无随机采样的配置，为参数选择提供了实证依据。

**Section sources**
- [biencoder_embedding_classification_decreased_0.01.py](file://bert/model_structure_with_different_decreased_random_rate/biencoder_embedding_classification_decreased_0.01.py#L201-L202)
- [biencoder_embedding_classification_concanated_together.py](file://bert/model_structure_with_simply_random_negative/biencoder_embedding_classification_concanated_together.py#L201-L202)

## 结论
随机采样策略作为一种简单而有效的负样本选择方法，在对抗性论点检索任务中发挥了重要作用。通过在BallTree搜索中引入可控的随机性，该策略成功地平衡了探索与利用的关系，防止了模型过早收敛于简单负例。其低计算开销的特点使其能够无缝集成到现有训练流程中，而动态调整随机率的机制则有效缓解了噪声引入的风险。

未来的工作可以进一步探索自适应随机率调整策略，根据模型的学习状态动态决定随机采样的强度。此外，结合困难负例挖掘与随机采样的混合策略也可能带来性能的进一步提升。